[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Below are selected projects showcasing business insight, storytelling, dashboards, and analytics workflows.\n\n\n\n\nOverview:\nAnalyzed global video game sales data to uncover trends in genre popularity, platform performance, and regional demand.\nProblem/Goal:\nGameCo aims to leverage data-driven insights to guide the development of new games and forecast their potential performance in the market.\nData:\nHistorical sales of video games dataset sourced from VGChartz. It tracks the total number of units of games sold (not financial figures) from 1980 to 2016. The numbers represent units sold in millions.\nMethodology:\nData Cleaning: The GameCo dataset was thoroughly cleaned to ensure accuracy and consistency before analysis. Missing values were handled through mean imputation for regional sales columns, while the NA Units column and an empty final row were removed. Duplicates and blank entries were corrected, special characters in game names were standardized, and all sales figures were rounded to two decimal places.\nData Processing: Descriptive statistical techniques were applied to summarize sales data across genres, publishers, and regions. Data was grouped and aggregated using Excel and Pivot tables to identify sales trends, regional performance variations, and changes in game popularity over time. Visualizations such as bar charts and line graphs were created to highlight key insights.\nKey Focus: - Sales trends across regions and time\n- Top-performing genres and platforms\n- Data-driven recommendations for product strategy\nTools: Excel, Tableau, PowerPoint, SQL\n\n\n\n\n\nOverview:\nConducted exploratory and predictive analysis on influenza mortality data to support healthcare staffing and resource allocation decisions.\nGoal:\nTo use influenza trends to provide insights to help a medical staffing agency that provides temporary workers to clinics and hospitals on an as-needed basis.\nData:\nInfluenza deaths by geography sourced from U.S. Centers for Disease Control and Prevention, and population data by geography, time, age, and gender sourced from US Census Bureau. The datasets span from 2009 to 2017 and provide detailed counts of influenza-related deaths by state and age group.\nMethodology:\nData Cleaning: Performed comprehensive data cleaning on both datasets, including profiling and integrity checks to ensure consistency and accuracy. This included renaming and formatting variables, validating data types, and addressing missing values.\nData Processing: Integrated flu mortality data with state population data to build a unified, analysis-ready dataset. Conducted statistical hypothesis testing to assess the relationship between the vulnerable population (age 65+) and flu-related deaths across states. Developed a range of Tableau visualizations including temporal trends, forecasts, statistical plots, and spatial maps to uncover key patterns and highlight geographic disparities.\nKey Focus: - State-level flu mortality trends\n- Risk analysis for populations aged 65+\n- Interactive dashboards for planning\nTools: Excel, Tableau, Python\nLinks:\n- GitHub: Repository\n\n\n\n\n\nOverview:\nPerformed advanced SQL analysis on customer, rental, and payment data to support Rockbuster‚Äôs transition into online video rentals.\nGoal:\nRockbuster Stealth LLC aims to transition into the online video rental market to remain competitive against streaming giants like Netflix and Amazon Prime. The management team seeks data-driven insights from existing customer and rental data to guide strategic decisions for their business plan.\nData:\nDataset containing information about Film Inventory, Customers Information, and Payment Transactions sourced from Rockbuster‚Äôs internal records.\nMethodology:\nData Cleaning: All available data was loaded into a relational database management system (RDBMS) to ensure proper storage, structure, and integration across tables. SQL was used extensively to explore and clean the data, filtering out irrelevant or duplicate records, and handling missing or incorrect values.\nData Processing: DbVisualizer was used to create an entity relationship diagram (ERD) to clearly understand table connections and database structure. Data from multiple tables was combined using SQL joins to build complete views of customer, film, rental, and payment information. Subqueries were employed to extract targeted metrics, and Common Table Expressions (CTEs) were used to simplify multi-step transformations and improve query readability.\nKey Focus: - Database design & ER diagrams\n- Customer and revenue analysis\n- Regional performance insights\nTools: PostgreSQL, SQL, Tableau, Power BI, DbVisualizer\nLinks:\n- GitHub: Repository\n\n\n\n\n\nOverview:\nAnalyzed large-scale grocery transaction data to uncover customer purchasing patterns and segmentation insights.\nGoal:\nInstacart, an app-based online grocery platform, aims to analyze its sales data more deeply to uncover patterns and provide insights for improved customer segmentation and strategic decision-making.\nData:\nInstacart Online Grocery Shopping Dataset (2017) sourced from Kaggle, along with a Customer Dataset provided specifically for analysis in this project.\nMethodology:\nData Cleaning: All Instacart datasets were imported into Python and prepared for analysis through careful data cleaning. This included wrangling and sub-setting to focus on relevant data, performing consistency checks to correct duplicates, handle missing values, and standardize formats.\nData Processing: Processed all datasets to generate meaningful insights. This included combining datasets for a comprehensive view, deriving new variables such as loyalty_flag and busiest_period_of_day, and grouping and aggregating data to summarize trends across products, customers, and departments.\nKey Focus: - Customer loyalty and ordering behavior\n- Product and department performance\n- Marketing and sales optimization insights\nTools: Python (Pandas, NumPy), Jupyter, Excel\nLinks:\n- GitHub: Repository\n\n\n\n\n\nOverview:\nAnalyzed over 260,000 incidents to uncover temporal, geographic, and severity patterns using exploratory analysis, clustering, and time-series methods.\nGoal:\nGun violence is a critical public health issue in the United States. This analysis investigates gun violence trends from 2013-2018 to uncover actionable insights and support the development of potential solutions for creating a safer, non-violent society.\nData:\nGun violence dataset containing over 260,000 recorded gun violence incidents spanning January 2013 to March 2018. Each entry includes up to 29 variables such as incident date, city and state, geographic coordinates, number of fatalities and injuries, weapon details, and participant demographics.\nMethodology:\nData Cleaning: The dataset initially contained 239,677 rows and 29 variables. Data cleaning began with validating and correcting variable data types, followed by a systematic assessment of missing values. Variables with excessive missing data (&gt;70%) or low analytical relevance were removed. Variables that were analytically useful but incomplete were retained, with missing categorical values filled as ‚ÄúUnknown‚Äù and numerical values imputed using the median.\nData Processing: The dataset was systematically processed and analyzed using multiple analytical approaches. This included the creation of geographic visualizations to examine spatial distribution across states, time-series analysis to uncover temporal trends and structural shifts, and both supervised and unsupervised machine learning techniques to identify patterns and classify incident types.\nKey Focus: - Time-series trends and seasonality\n- Geographic distribution and hotspots\n- Clustering of incident types and severity patterns\nTools: Python, Tableau, Jupyter\nLinks:\n- GitHub: Repository"
  },
  {
    "objectID": "projects.html#gameco-video-game-sales-analysis",
    "href": "projects.html#gameco-video-game-sales-analysis",
    "title": "Projects",
    "section": "",
    "text": "Overview:\nAnalyzed global video game sales data to uncover trends in genre popularity, platform performance, and regional demand.\nProblem/Goal:\nGameCo aims to leverage data-driven insights to guide the development of new games and forecast their potential performance in the market.\nData:\nHistorical sales of video games dataset sourced from VGChartz. It tracks the total number of units of games sold (not financial figures) from 1980 to 2016. The numbers represent units sold in millions.\nMethodology:\nData Cleaning: The GameCo dataset was thoroughly cleaned to ensure accuracy and consistency before analysis. Missing values were handled through mean imputation for regional sales columns, while the NA Units column and an empty final row were removed. Duplicates and blank entries were corrected, special characters in game names were standardized, and all sales figures were rounded to two decimal places.\nData Processing: Descriptive statistical techniques were applied to summarize sales data across genres, publishers, and regions. Data was grouped and aggregated using Excel and Pivot tables to identify sales trends, regional performance variations, and changes in game popularity over time. Visualizations such as bar charts and line graphs were created to highlight key insights.\nKey Focus: - Sales trends across regions and time\n- Top-performing genres and platforms\n- Data-driven recommendations for product strategy\nTools: Excel, Tableau, PowerPoint, SQL"
  },
  {
    "objectID": "projects.html#preparing-for-influenza-season",
    "href": "projects.html#preparing-for-influenza-season",
    "title": "Projects",
    "section": "",
    "text": "Overview:\nConducted exploratory and predictive analysis on influenza mortality data to support healthcare staffing and resource allocation decisions.\nGoal:\nTo use influenza trends to provide insights to help a medical staffing agency that provides temporary workers to clinics and hospitals on an as-needed basis.\nData:\nInfluenza deaths by geography sourced from U.S. Centers for Disease Control and Prevention, and population data by geography, time, age, and gender sourced from US Census Bureau. The datasets span from 2009 to 2017 and provide detailed counts of influenza-related deaths by state and age group.\nMethodology:\nData Cleaning: Performed comprehensive data cleaning on both datasets, including profiling and integrity checks to ensure consistency and accuracy. This included renaming and formatting variables, validating data types, and addressing missing values.\nData Processing: Integrated flu mortality data with state population data to build a unified, analysis-ready dataset. Conducted statistical hypothesis testing to assess the relationship between the vulnerable population (age 65+) and flu-related deaths across states. Developed a range of Tableau visualizations including temporal trends, forecasts, statistical plots, and spatial maps to uncover key patterns and highlight geographic disparities.\nKey Focus: - State-level flu mortality trends\n- Risk analysis for populations aged 65+\n- Interactive dashboards for planning\nTools: Excel, Tableau, Python\nLinks:\n- GitHub: Repository"
  },
  {
    "objectID": "projects.html#rockbuster-stealth-data-analysis",
    "href": "projects.html#rockbuster-stealth-data-analysis",
    "title": "Projects",
    "section": "",
    "text": "Overview:\nPerformed advanced SQL analysis on customer, rental, and payment data to support Rockbuster‚Äôs transition into online video rentals.\nGoal:\nRockbuster Stealth LLC aims to transition into the online video rental market to remain competitive against streaming giants like Netflix and Amazon Prime. The management team seeks data-driven insights from existing customer and rental data to guide strategic decisions for their business plan.\nData:\nDataset containing information about Film Inventory, Customers Information, and Payment Transactions sourced from Rockbuster‚Äôs internal records.\nMethodology:\nData Cleaning: All available data was loaded into a relational database management system (RDBMS) to ensure proper storage, structure, and integration across tables. SQL was used extensively to explore and clean the data, filtering out irrelevant or duplicate records, and handling missing or incorrect values.\nData Processing: DbVisualizer was used to create an entity relationship diagram (ERD) to clearly understand table connections and database structure. Data from multiple tables was combined using SQL joins to build complete views of customer, film, rental, and payment information. Subqueries were employed to extract targeted metrics, and Common Table Expressions (CTEs) were used to simplify multi-step transformations and improve query readability.\nKey Focus: - Database design & ER diagrams\n- Customer and revenue analysis\n- Regional performance insights\nTools: PostgreSQL, SQL, Tableau, Power BI, DbVisualizer\nLinks:\n- GitHub: Repository"
  },
  {
    "objectID": "projects.html#instacart-grocery-basket-analysis",
    "href": "projects.html#instacart-grocery-basket-analysis",
    "title": "Projects",
    "section": "",
    "text": "Overview:\nAnalyzed large-scale grocery transaction data to uncover customer purchasing patterns and segmentation insights.\nGoal:\nInstacart, an app-based online grocery platform, aims to analyze its sales data more deeply to uncover patterns and provide insights for improved customer segmentation and strategic decision-making.\nData:\nInstacart Online Grocery Shopping Dataset (2017) sourced from Kaggle, along with a Customer Dataset provided specifically for analysis in this project.\nMethodology:\nData Cleaning: All Instacart datasets were imported into Python and prepared for analysis through careful data cleaning. This included wrangling and sub-setting to focus on relevant data, performing consistency checks to correct duplicates, handle missing values, and standardize formats.\nData Processing: Processed all datasets to generate meaningful insights. This included combining datasets for a comprehensive view, deriving new variables such as loyalty_flag and busiest_period_of_day, and grouping and aggregating data to summarize trends across products, customers, and departments.\nKey Focus: - Customer loyalty and ordering behavior\n- Product and department performance\n- Marketing and sales optimization insights\nTools: Python (Pandas, NumPy), Jupyter, Excel\nLinks:\n- GitHub: Repository"
  },
  {
    "objectID": "projects.html#gun-violence-in-the-u.s.-20132018",
    "href": "projects.html#gun-violence-in-the-u.s.-20132018",
    "title": "Projects",
    "section": "",
    "text": "Overview:\nAnalyzed over 260,000 incidents to uncover temporal, geographic, and severity patterns using exploratory analysis, clustering, and time-series methods.\nGoal:\nGun violence is a critical public health issue in the United States. This analysis investigates gun violence trends from 2013-2018 to uncover actionable insights and support the development of potential solutions for creating a safer, non-violent society.\nData:\nGun violence dataset containing over 260,000 recorded gun violence incidents spanning January 2013 to March 2018. Each entry includes up to 29 variables such as incident date, city and state, geographic coordinates, number of fatalities and injuries, weapon details, and participant demographics.\nMethodology:\nData Cleaning: The dataset initially contained 239,677 rows and 29 variables. Data cleaning began with validating and correcting variable data types, followed by a systematic assessment of missing values. Variables with excessive missing data (&gt;70%) or low analytical relevance were removed. Variables that were analytically useful but incomplete were retained, with missing categorical values filled as ‚ÄúUnknown‚Äù and numerical values imputed using the median.\nData Processing: The dataset was systematically processed and analyzed using multiple analytical approaches. This included the creation of geographic visualizations to examine spatial distribution across states, time-series analysis to uncover temporal trends and structural shifts, and both supervised and unsupervised machine learning techniques to identify patterns and classify incident types.\nKey Focus: - Time-series trends and seasonality\n- Geographic distribution and hotspots\n- Clustering of incident types and severity patterns\nTools: Python, Tableau, Jupyter\nLinks:\n- GitHub: Repository"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "Get in Touch\nI‚Äôm open to discussing data analytics opportunities, collaborations, or interesting projects.\n\nüìß Email: oluwole.akorede@yahoo.com\n\nüîó LinkedIn: https://www.linkedin.com/in/oluwole-oni-193b49163\n\nüíª GitHub: https://github.com/Oluwole-Oni"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a Data Analyst with a foundation in Mechanical Engineering, holding a Bachelor of Engineering degree and an Automotive Master Certificate from the Community College of Baltimore County. Over the years, I worked extensively with diagnostic data, performance metrics, and system troubleshooting, experiences that shaped my analytical thinking and attention to detail.\nMy transition into data analytics was driven by a growing interest in how data patterns, trends, and visualizations can influence strategic decisions. Through the CareerFoundry Data Analytics program, I gained hands-on experience delivering end-to-end analytics projects using Python, SQL, Tableau, Excel, and Power BI.\nWhat sets me apart is the combination of engineering discipline and data storytelling. I‚Äôm comfortable working with complex datasets and equally comfortable translating insights for non-technical stakeholders. My goal is always to connect analysis to business value, operational efficiency, and informed decision-making.\nI am currently open to full-time Data Analyst opportunities and enjoy collaborating with teams that value clarity, evidence-based strategy, and continuous improvement.\n\n\n\n\nMechanical Engineering mindset applied to analytics problem-solving\n\nEnd-to-end projects: cleaning ‚Üí EDA ‚Üí insights ‚Üí dashboards ‚Üí recommendations\n\nStrong communication for technical and non-technical stakeholders"
  },
  {
    "objectID": "about.html#highlights",
    "href": "about.html#highlights",
    "title": "About",
    "section": "",
    "text": "Mechanical Engineering mindset applied to analytics problem-solving\n\nEnd-to-end projects: cleaning ‚Üí EDA ‚Üí insights ‚Üí dashboards ‚Üí recommendations\n\nStrong communication for technical and non-technical stakeholders"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Data Analyst | Business Intelligence | Data Storytelling\nTurning complex data into clear, actionable business insights\n\n\n\n\nI‚Äôm a Data Analyst with a Mechanical Engineering background, bringing a structured, analytical mindset to data-driven decision-making. My journey from engineering and automotive diagnostics into analytics allows me to approach data problems with precision, curiosity, and a strong focus on real-world impact.\nI specialize in SQL, Python, Tableau, Excel, and Power BI, working across the full analytics lifecycle‚Äîfrom data cleaning and validation to insight generation and storytelling. I don‚Äôt just analyze numbers; I focus on why patterns exist, what they mean for the business, and how insights can drive smarter decisions.\n\n\n\n\n\nData Analytics & Business Insight\n\nData Visualization & Storytelling\n\nExploratory Data Analysis (EDA)\n\nForecasting & Time Series Analysis\n\nStatistical Testing & Hypothesis Validation\n\nData Cleaning, Integration & ETL\n\n\n\n\n\nPython (Pandas, NumPy) ‚Ä¢ SQL (PostgreSQL) ‚Ä¢ Tableau ‚Ä¢ Power BI ‚Ä¢ Excel ‚Ä¢ Jupyter ‚Ä¢ Anaconda\n\n\n\n\nExplore projects covering sales analysis, customer behavior, forecasting, public health, and social impact, built using Python, SQL, Tableau, and Excel.\nüëâ Go to: Projects"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Home",
    "section": "",
    "text": "I‚Äôm a Data Analyst with a Mechanical Engineering background, bringing a structured, analytical mindset to data-driven decision-making. My journey from engineering and automotive diagnostics into analytics allows me to approach data problems with precision, curiosity, and a strong focus on real-world impact.\nI specialize in SQL, Python, Tableau, Excel, and Power BI, working across the full analytics lifecycle‚Äîfrom data cleaning and validation to insight generation and storytelling. I don‚Äôt just analyze numbers; I focus on why patterns exist, what they mean for the business, and how insights can drive smarter decisions."
  },
  {
    "objectID": "index.html#core-skills",
    "href": "index.html#core-skills",
    "title": "Home",
    "section": "",
    "text": "Data Analytics & Business Insight\n\nData Visualization & Storytelling\n\nExploratory Data Analysis (EDA)\n\nForecasting & Time Series Analysis\n\nStatistical Testing & Hypothesis Validation\n\nData Cleaning, Integration & ETL"
  },
  {
    "objectID": "index.html#tools",
    "href": "index.html#tools",
    "title": "Home",
    "section": "",
    "text": "Python (Pandas, NumPy) ‚Ä¢ SQL (PostgreSQL) ‚Ä¢ Tableau ‚Ä¢ Power BI ‚Ä¢ Excel ‚Ä¢ Jupyter ‚Ä¢ Anaconda"
  },
  {
    "objectID": "index.html#featured-work",
    "href": "index.html#featured-work",
    "title": "Home",
    "section": "",
    "text": "Explore projects covering sales analysis, customer behavior, forecasting, public health, and social impact, built using Python, SQL, Tableau, and Excel.\nüëâ Go to: Projects"
  }
]